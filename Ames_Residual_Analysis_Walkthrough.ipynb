{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries"
      ],
      "metadata": {
        "id": "znlEJXqDygno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load dependencies**"
      ],
      "metadata": {
        "id": "EgDha-rLylhu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi34ylT_QYdu"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "import pylab as py\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download data: 'black box' predictions by random forest**"
      ],
      "metadata": {
        "id": "Q1Rst2ZiQnZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of dataset - with predictions generated by linear regression\n",
        "URL = \"https://github.com/SHeidema/AMES-TRIDSA/blob/main/datasets/predictions_rf.csv?raw=true\"\n",
        "\n",
        "# Load dataset\n",
        "df_full = (pd.read_csv(URL))\n",
        "\n",
        "# Select first house, for which we will calculate a prediction interval\n",
        "df_0 = df_full.iloc[0]\n",
        "\n",
        "# Remove first house from full dataset\n",
        "df = df_full.drop(index=0)"
      ],
      "metadata": {
        "id": "dLmN79WQQmOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory data analysis"
      ],
      "metadata": {
        "id": "GRnmNmOtQ1qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Y_i$ be the real sale price, and $\\hat{Y_i}$ its predicted value. The residuals are defined as $\\varepsilon_i = Y_i - \\hat{Y_i}$"
      ],
      "metadata": {
        "id": "G_VGeveYV-4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate the residuals and its summary statistics**"
      ],
      "metadata": {
        "id": "X00BehPeRA7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate residuals\n",
        "df['Residuals'] = df['SalePrice'] - df['SalePricePrediction']"
      ],
      "metadata": {
        "id": "vP5C6IcATQ9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics\n",
        "\n",
        "# Mean\n",
        "mean = np.mean(df['Residuals'])\n",
        "print(f\"The estimated mean: {mean}\")\n",
        "\n",
        "# Median\n",
        "median = np.median(df['Residuals'])\n",
        "print(f\"The estimated median: {median}\")\n",
        "\n",
        "# Standard deviation, ddof=1 ensures the maximum likelihood estimate\n",
        "std = np.std(df['Residuals'], ddof = 1)\n",
        "print(f\"The estimated standard deviation is: {std}\")\n",
        "\n",
        "# Skewness\n",
        "skewness = scipy.stats.skew(df['Residuals'])\n",
        "print(f\"The estimated skewness is: {skewness}\")\n",
        "\n",
        "# Kurtosis\n",
        "kurtosis = scipy.stats.kurtosis(df['Residuals'])\n",
        "print(f\"The estimated kurtosis is: {kurtosis}\")\n"
      ],
      "metadata": {
        "id": "IRn2dXPwQwTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Investigate relationship between $Y_i$ and $\\hat{Y_i}$ through a scatterplot**"
      ],
      "metadata": {
        "id": "Kd_B4lGWqf1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot\n",
        "plt.scatter(df['SalePricePrediction'], df['SalePrice'])\n",
        "plt.xlabel('Sale Price Prediction')\n",
        "plt.ylabel('Sale Price')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5RW5cA2bqfgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Investigate normality of residuals through histogram**"
      ],
      "metadata": {
        "id": "zVkUfK5PUdIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram\n",
        "(xmin, xmax) = (-100000,100000)\n",
        "plt.hist(df['Residuals'], edgecolor='black', bins=30, range=(xmin, xmax), density=True)\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Fit a normal distribution\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = stats.norm.pdf(x, mean, std)\n",
        "\n",
        "plt.plot(x, p, 'k', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CB1UmPujUb75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have already seen earlier, the kurtosis is too high. What happens if we apply a transformation: scaling the residuals by the prediction?"
      ],
      "metadata": {
        "id": "6DuBuuNapZC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate scaled residuals\n",
        "df['ScaledResiduals'] = (df['SalePrice'] - df['SalePricePrediction']) / df['SalePricePrediction']\n",
        "plt.xlabel('Scaled Residuals')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "\n",
        "# Fit a normal distribution\n",
        "(xmin, xmax) = (-1,1)\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = stats.norm.pdf(x, np.mean(df['ScaledResiduals']), np.std(df['ScaledResiduals']))\n",
        "\n",
        "# Histogram\n",
        "plt.hist(df['ScaledResiduals'], edgecolor='black', bins=30, range=(xmin, xmax), density=True)\n",
        "\n",
        "plt.plot(x, p, 'k', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yDqu6GDWoeB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These scaled residuals already look more like a normal distribution."
      ],
      "metadata": {
        "id": "R2IktLDupat4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection, prediction intervals"
      ],
      "metadata": {
        "id": "DmMpk7ZibjZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estimate parameters of the following model using maximum likelihood estimation**"
      ],
      "metadata": {
        "id": "BCuj1AQ4Ydvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that by rewriting the normality assumption on the residuals, one can obtain model 1. <br>\n",
        "Model 1: <br>\n",
        "$Y_i \\sim N\\left(\\hat{Y_i}, \\sigma^2\\right)$\n"
      ],
      "metadata": {
        "id": "jl8XBb7fX0zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neg_loglike_model1(theta):\n",
        "    mu = df['SalePricePrediction']\n",
        "    sd = theta[0]\n",
        "    return -1*stats.norm(mu, sd).logpdf(df['SalePrice']).sum()\n",
        "\n",
        "\n",
        "theta_start = np.array([1])\n",
        "\n",
        "res = scipy.optimize.minimize(neg_loglike_model1, theta_start , method = 'Nelder-Mead',\n",
        "           options={'disp': True})\n",
        "\n",
        "\n",
        "sigma_mle1 = float(res['x'])\n",
        "loglik1 = - res.fun\n",
        "aic1 = 2*len(theta_start) - 2* loglik1\n",
        "\n",
        "print(f\"The numerical MLE for sigma: {sigma_mle1}\")\n",
        "print(f\"The loglikelihood: {loglik1}\")\n",
        "print(f\"The AIC: {aic1}\")"
      ],
      "metadata": {
        "id": "LDhorGXpYL9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen in the slides, given an estimated distribution function $\\hat{F}$, we can use the inverse cumulative distribution function (quantile) $\\hat{F}^{-1}$ to calculate the upper and lower prediction limits:<br>\n",
        "\n",
        "$UPL = \\hat{F}^{-1}(1-\\alpha/2)$<br>\n",
        "$LPL = \\hat{F}^{-1}(\\alpha/2)$<br>\n",
        "\n",
        "Calculate the 95% prediction interval (i.e. $\\alpha=0.05$) for $Y_0$ given $\\hat{Y}_0$ based on the quantiles of model 1. Use the numerically estimated parameter."
      ],
      "metadata": {
        "id": "7wfh2Rl4uvkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UPL1 = stats.norm.ppf(0.975, loc=df_0['SalePricePrediction'], scale=sigma_mle1)\n",
        "LPL1 = stats.norm.ppf(0.025, loc=df_0['SalePricePrediction'], scale=sigma_mle1)\n",
        "\n",
        "print(f\"The predicted sale price is: {df_0['SalePricePrediction']}\" )\n",
        "print(f\"With prediction interval:\" )\n",
        "print(LPL1, UPL1)\n"
      ],
      "metadata": {
        "id": "kA61Yrjpu4m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we can verify that the real sale price $(Y_0 = â‚¬215.000)$ is within the prediction interval."
      ],
      "metadata": {
        "id": "mHVEYyTN1TeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estimate parameters of the following models using maximum likelihood estimation**<br>\n",
        "Other choices are possible to model the variance, for instance a variance which scales in the predicted sale price. <br>\n",
        "Model 2: <br>\n",
        "$Y_i \\sim N\\left(\\hat{Y_i}, \\left(\\hat{Y_i}\\sigma\\right)^2\\right)$\n",
        "\n",
        "Model 3: <br>\n",
        "$Y_i \\sim N\\left(\\hat{Y_i}, \\tau^2 + \\left(\\hat{Y_i}\\sigma\\right)^2\\right)$\n"
      ],
      "metadata": {
        "id": "cSN9CkYYbJPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neg_loglike_model2(theta):\n",
        "    mu = df['SalePricePrediction']\n",
        "    sd = theta[0]*(df['SalePricePrediction'])\n",
        "    return -1*stats.norm(mu, sd).logpdf(df['SalePrice']).sum()\n",
        "\n",
        "theta_start = np.array([1])\n",
        "res = scipy.optimize.minimize(neg_loglike_model2, theta_start, method = 'Nelder-Mead',\n",
        "           options={'disp': True})\n",
        "sigma_mle2 = float(res['x'])\n",
        "loglik2 = - res.fun\n",
        "aic2 = 2*len(theta_start) - 2* loglik2\n",
        "\n",
        "print(f\"The theoretical MLE for sigma: {np.std(df['SalePrice']/df['SalePricePrediction'])}\")\n",
        "print(f\"The numerical MLE for sigma: {sigma_mle2}\")\n",
        "print(f\"The loglikelihood: {loglik2}\")\n",
        "print(f\"The AIC: {aic2}\")"
      ],
      "metadata": {
        "id": "Gf7-KuUPPlaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neg_loglike_model3(theta):\n",
        "    mu = df['SalePricePrediction']\n",
        "    sd = np.sqrt(theta[0]**2 + (theta[1]*df['SalePricePrediction'])**2)\n",
        "    return -1*stats.norm(mu, sd).logpdf(df['SalePrice']).sum()\n",
        "\n",
        "theta_start = np.array([1,1])\n",
        "res = scipy.optimize.minimize(neg_loglike_model3, theta_start, method = 'Nelder-Mead',\n",
        "           options={'disp': True})\n",
        "\n",
        "tau_mle3 = float(res['x'][0])\n",
        "sigma_mle3 = float(res['x'][1])\n",
        "\n",
        "loglik3 = - res.fun\n",
        "aic3 = 2*len(theta_start) - 2* loglik3\n",
        "\n",
        "print(f\"The numerical MLE for tau: {tau_mle3}\")\n",
        "print(f\"The numerical MLE for sigma: {sigma_mle3}\")\n",
        "print(f\"The loglikelihood: {loglik3}\")\n",
        "print(f\"The AIC: {aic3}\")"
      ],
      "metadata": {
        "id": "4BiU7lJMRuY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_0_hat = df_0['SalePricePrediction']\n",
        "y_0 =  df_0['SalePrice']\n",
        "\n",
        "# Prediction interval for model 2\n",
        "UPL2 = stats.norm.ppf(0.975, loc=y_0_hat, scale=sigma_mle2*y_0_hat)\n",
        "LPL2 = stats.norm.ppf(0.025, loc=y_0_hat, scale=sigma_mle2*y_0_hat)\n",
        "\n",
        "# Prediction interval for model 3\n",
        "UPL3 = stats.norm.ppf(0.975, loc=y_0_hat, scale=np.sqrt(tau_mle3**2 + (sigma_mle3*y_0_hat)**2))\n",
        "LPL3 = stats.norm.ppf(0.025, loc=y_0_hat, scale=np.sqrt(tau_mle3**2 + (sigma_mle3*y_0_hat)**2))\n",
        "\n",
        "print(f\"The predicted sale price is: {y_0_hat}\" )\n",
        "print(f\"Prediction interval of model 2:\" )\n",
        "print(np.round(LPL2), np.round(UPL2))\n",
        "\n",
        "print(f\"Prediction interval of model 3:\" )\n",
        "print(np.round(LPL3), np.round(UPL3))\n"
      ],
      "metadata": {
        "id": "OVLwzCu9e29l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now calculate prediction intervals given any predicted sale price according to the three methods:"
      ],
      "metadata": {
        "id": "g93lcN0fgIKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xx = np.arange(1,round(np.max(df['SalePricePrediction']))+1e5)\n",
        "\n",
        "# Prediction intervals for model 1\n",
        "upper_1 = stats.norm.ppf(0.975, loc=xx, scale=sigma_mle1)\n",
        "lower_1 = stats.norm.ppf(0.025, loc=xx, scale=sigma_mle1)\n",
        "\n",
        "# Prediction intervals for model 2\n",
        "upper_2 = stats.norm.ppf(0.975, loc=xx, scale=sigma_mle2*xx)\n",
        "lower_2 = stats.norm.ppf(0.025, loc=xx, scale=sigma_mle2*xx)\n",
        "\n",
        "# Prediction intervals for model 3\n",
        "upper_3 = stats.norm.ppf(0.975, loc=xx, scale=np.sqrt(tau_mle3**2 + (sigma_mle3*xx)**2))\n",
        "lower_3 = stats.norm.ppf(0.025, loc=xx, scale=np.sqrt(tau_mle3**2 + (sigma_mle3*xx)**2))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.plot(df['SalePricePrediction'], df['SalePrice'], \"o\", label=\"Data\",alpha=0.7)\n",
        "# Model 1\n",
        "ax.plot(xx, upper_1, \"r--\", label=\"Model 1\", linewidth= 2)\n",
        "ax.plot(xx, lower_1, \"r--\",  linewidth= 2)\n",
        "\n",
        "\n",
        "# Model 2\n",
        "ax.plot(xx, upper_2, \"k--\", label=\"Model 2\", linewidth= 2, alpha=.8)\n",
        "ax.plot(xx, lower_2, \"k--\",  linewidth= 2, alpha=.8)\n",
        "\n",
        "# Model 3\n",
        "ax.plot(xx, upper_3, \"b--\", label=\"Model 3\", linewidth= 2)\n",
        "ax.plot(xx, lower_3, \"b--\",  linewidth= 2)\n",
        "\n",
        "ax.set_xlim(1,round(np.max(df['SalePricePrediction']))+1e5)\n",
        "ax.set_xlabel('Sale Price Prediction')\n",
        "ax.set_ylabel('Sale Price')\n",
        "ax.legend(loc=\"best\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "fJVd0949gVHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But which model is the most adequate?\n",
        "\n",
        "**Determine best model by comparing the AIC and applying the Likelihood Ratio Test.**"
      ],
      "metadata": {
        "id": "zvyGXLf4t_Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The AIC of model 1: {aic1}\")\n",
        "print(f\"The AIC of model 2: {aic2}\")\n",
        "print(f\"The AIC of model 3: {aic3}\")\n",
        "\n",
        "print(f\"The log-likelihood of model 1: {loglik1}\")\n",
        "print(f\"The log-likelihood of model 2: {loglik2}\")\n",
        "print(f\"The log-likelihood of model 3: {loglik3}\")"
      ],
      "metadata": {
        "id": "1sBjKEXviLQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 3 has the lowest AIC, implying that it is most adequate. <br>\n",
        "Note that model 2 has a higher likelihood than model 1. Since their number of parameters is the same, model 2 outperforms model 1. Let us now compare model 2 and model 3 using the Likelihood Ratio Test."
      ],
      "metadata": {
        "id": "DTloowvUigFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Likelihood ratio test\n",
        "L = -2*(loglik2 - loglik3)\n",
        "\n",
        "# P value\n",
        "p_val = 1 - stats.chi2.cdf(L, 1)\n",
        "\n",
        "print(f\"The likelihood ratio test statistic equals: {L}\")\n",
        "print(f\"The corresponding p-value equals: {p_val}\")"
      ],
      "metadata": {
        "id": "0P6lNPESttCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We conclude that model 3 is the most adequate model."
      ],
      "metadata": {
        "id": "zrgf_sKNVcfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confidence intervals do not equal prediction intervals"
      ],
      "metadata": {
        "id": "gao_m3BB085y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given our models, estimating a confidence interval for the mean of $Y_i$ is not interesting because it is assumed to equal exactly $\\hat{Y}_i$. <br>\n",
        "\n",
        "Therefore, in this simulation assume that data is normally distributed with mean 2 and standard deviation 1. Notice that the confidence interval of the mean, which is assumed to contain the real mean 95% of the time, is much smaller than the prediction interval, which is assumed to contain 95% of observtions."
      ],
      "metadata": {
        "id": "R9TMGAl8kMzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate normally distributed data\n",
        "np.random.seed(1)\n",
        "true_mean = 2\n",
        "data = np.random.normal(loc=true_mean, scale=1, size=100)\n",
        "\n",
        "# Calculate the sample mean and standard deviation\n",
        "sample_mean = np.mean(data)\n",
        "sample_std = np.std(data, ddof=1)\n",
        "\n",
        "# Set the confidence level\n",
        "confidence_level = 0.95\n",
        "\n",
        "# Calculate the confidence interval\n",
        "z_critical = stats.norm.ppf(1 - (1 - confidence_level) / 2)\n",
        "confidence_interval = z_critical * sample_std / np.sqrt(len(data))\n",
        "\n",
        "# Calculate the prediction interval\n",
        "pred_lower = stats.norm.ppf(1 - (1 - confidence_level) / 2, loc=sample_mean, scale=sample_std)\n",
        "pred_upper = stats.norm.ppf((1 - confidence_level) / 2, loc=sample_mean, scale=sample_std)\n",
        "\n",
        "# Plotting the histogram\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.hist(data, bins=30, density=True, edgecolor='black', alpha=0.7, label='Data')\n",
        "\n",
        "# Plotting the sample mean\n",
        "ax.axvline(x=sample_mean, color='red', linestyle='--', label='Sample Mean')\n",
        "\n",
        "# Plotting the confidence interval\n",
        "ax.axvline(x=sample_mean - confidence_interval, color='green', linestyle='--', label='Confidence Interval')\n",
        "ax.axvline(x=sample_mean + confidence_interval, color='green', linestyle='--')\n",
        "\n",
        "# Plotting the prediction interval\n",
        "ax.axvline(x=pred_lower, color='orange', linestyle='--', label='Prediction Interval')\n",
        "ax.axvline(x=pred_upper, color='orange', linestyle='--')\n",
        "\n",
        "ax.set_xlabel('Value')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.legend(loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f88xy11dszF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "mNaJKWM5VwoU"
      }
    }
  ]
}