{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install vegafusion"
      ],
      "metadata": {
        "id": "mIZVWXkAVLZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290eaaa0-9ce4-4e26-eb7d-3b746b83fc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vegafusion in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from vegafusion) (4.2.2)\n",
            "Requirement already satisfied: pyarrow>=5 in /usr/local/lib/python3.10/dist-packages (from vegafusion) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from vegafusion) (1.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vegafusion) (5.9.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->vegafusion) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->vegafusion) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->vegafusion) (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->vegafusion) (1.22.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->vegafusion) (0.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vegafusion) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vegafusion) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->vegafusion) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->vegafusion) (0.19.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->vegafusion) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair>=4.2.0->vegafusion) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import altair as alt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import vegafusion as vf\n",
        "import random\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
        "from sklearn.utils import estimator_html_repr\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# check versions\n",
        "for library in [alt, pd, sklearn, vf]:\n",
        "    print(f\"using {library.__name__} version {library.__version__}\")\n",
        "\n",
        "# Using Altair for data visualization\n",
        "# enable VegaFusion to generate plots with more than 5,000 records\n",
        "vf.enable()\n",
        "\n",
        "# setting to visualize sklearn pipelines\n",
        "# see https://towardsdatascience.com/are-you-using-pipeline-in-scikit-learn-ac4cd85cb27f\n",
        "sklearn.set_config(display=\"diagram\")\n",
        "\n",
        "# dataset\n",
        "URL = \"https://github.com/jads-nl/discover-projects/blob/main/ames-housing/AmesHousing.csv?raw=true\"\n",
        "\n",
        "# leaderboard to compare our results\n",
        "LEADERBOARD = \"https://github.com/jads-nl/discover-projects/blob/main/ames-housing/house-prices-advanced-regression-techniques-publicleaderboard.csv?raw=true\"\n",
        "\n",
        "# fill-value for missings in categorical variables\n",
        "MISSING = \"missing\"\n",
        "NONE = \"not present\""
      ],
      "metadata": {
        "id": "GFteB511SYl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_column_names(s):\n",
        "    return s.replace(\" \", \"\")\n",
        "\n",
        "\n",
        "def na_means_none(df):\n",
        "    cols_na_means_none = [\n",
        "        \"Alley\",\n",
        "        \"BsmtQual\",\n",
        "        \"BsmtCond\",\n",
        "        \"BsmtFinType1\",\n",
        "        \"BsmtFinType2\",\n",
        "        \"FireplaceQu\",\n",
        "        \"GarageType\",\n",
        "        \"GarageFinish\",\n",
        "        \"GarageQual\",\n",
        "        \"GarageCond\",\n",
        "        \"PoolQC\",\n",
        "        \"Fence\",\n",
        "        \"MiscFeature\",\n",
        "    ]\n",
        "\n",
        "    df.loc[:, cols_na_means_none] = df.loc[:, cols_na_means_none].fillna(value=NONE)\n",
        "    return df\n",
        "\n",
        "\n",
        "def optimize_memory(df):\n",
        "    # objects to categorical\n",
        "    df[df.select_dtypes(include=\"object\").columns] = df.select_dtypes(\n",
        "        include=\"object\"\n",
        "    ).astype(\"category\")\n",
        "\n",
        "    # convert integers to smallest unsigned integer and floats to smallest\n",
        "    for old, new in [(\"integer\", \"unsigned\"), (\"float\", \"float\")]:\n",
        "        for col in df.select_dtypes(include=old).columns:\n",
        "            df[col] = pd.to_numeric(df[col], downcast=new)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df = (\n",
        "    pd.read_csv(URL)\n",
        "    .rename(columns=standardize_column_names)\n",
        "    .pipe(na_means_none)\n",
        "    .pipe(optimize_memory)\n",
        ")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "lYAbXz_UVGeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see which columns have NAs, you could make this into a small utility function\n",
        "def na_per_columns(df):\n",
        "    \"\"\"Calculates nulls per column\"\"\"\n",
        "    nulls = df.isnull().sum()\n",
        "    return nulls[nulls != 0].sort_values(ascending=False)\n",
        "\n",
        "\n",
        "cols_with_nulls = na_per_columns(df)\n",
        "cols_with_nulls"
      ],
      "metadata": {
        "id": "WGgzSEWTVQVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_outliers = df.query(\"GrLivArea < 4000\")\n",
        "df_no_outliers.shape"
      ],
      "metadata": {
        "id": "e02xH288Va_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate which columns have large percentage missing values, say 20%\n",
        "cols_to_drop = (cols_with_nulls[cols_with_nulls / len(df) > 0.2] / len(df)).index\n",
        "cols_with_nulls[cols_to_drop]"
      ],
      "metadata": {
        "id": "CjqY3jxjVV7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no data snooping - this function does random sampling\n",
        "random.seed(123)\n",
        "df_train, df_test = train_test_split(df_no_outliers, test_size=0.3)"
      ],
      "metadata": {
        "id": "rqZAlJqsVYMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare X and y, using log(SalePrice) throughout\n",
        "X = df_train[df_train.columns.difference(cols_to_drop).drop(\"SalePrice\")]\n",
        "y = np.log(df_train.SalePrice)\n",
        "\n",
        "# same for test set\n",
        "X_test = df_test[df_test.columns.difference(cols_to_drop).drop(\"SalePrice\")]\n",
        "y_test = np.log(df_test.SalePrice)\n",
        "\n",
        "# read https://numpy.org/doc/stable/reference/arrays.scalars.html\n",
        "# for understanding NumPy dtype hierarchy\n",
        "cat_cols = X.select_dtypes(include=\"category\").columns\n",
        "num_cols = X.select_dtypes(include=\"number\").columns"
      ],
      "metadata": {
        "id": "Id6R9PFJVcvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need to explicitly define categories for pipeline and add MISSING category\n",
        "categories = [df[col].cat.categories.to_list() for col in cat_cols]\n",
        "for cat in categories:\n",
        "    cat.append(MISSING)\n",
        "\n",
        "# combine all preprocessing for cat_cols in one pipeline\n",
        "preprocess_cat_cols = make_pipeline(\n",
        "    SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=MISSING),\n",
        "    OneHotEncoder(categories=categories),\n",
        ")\n",
        "\n",
        "# same for num_cols\n",
        "preprocess_num_cols = make_pipeline(\n",
        "    SimpleImputer(missing_values=np.nan, strategy=\"median\"), StandardScaler()\n",
        ")\n",
        "\n",
        "# compose dataset with make_column_transformer\n",
        "prepare_linear = make_column_transformer(\n",
        "    (preprocess_num_cols, num_cols), (preprocess_cat_cols, cat_cols), remainder=\"drop\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "FevTfljpWAMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare X and y, using log(SalePrice) throughout\n",
        "X_full = df_no_outliers[df_no_outliers.columns.difference(cols_to_drop).drop(\"SalePrice\")]\n",
        "y_full = np.log(df_no_outliers.SalePrice)\n",
        "\n",
        "\n",
        "# Make pipeline\n",
        "ols = make_pipeline(prepare_linear, LinearRegression())\n",
        "\n",
        "# Fit model on entire dataset\n",
        "ols.fit(X_full,y_full)\n",
        "\n",
        "\n",
        "# Get estimates\n",
        "y_hat = ols.predict(X_full)\n",
        "\n",
        "\n",
        "# Adjust object type\n",
        "\n",
        "y_full_col = pd.DataFrame({'SalePrice': np.exp(y_full)}, index = y_full.index)\n",
        "\n",
        "y_hat_col = pd.DataFrame({'SalePricePrediction': np.exp(y_hat)}, index = y_full.index)\n",
        "\n",
        "\n",
        "# Concatenate X_full, y_full_col, and y_hat_col\n",
        "df_combined = pd.concat([X_full, y_full_col, y_hat_col], axis=1)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "df_combined.to_csv('predictions_lr.csv', encoding = 'utf-8-sig')\n",
        "files.download('predictions_lr.csv')"
      ],
      "metadata": {
        "id": "xxUom22QV956"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_cat_cols_2 = make_pipeline(\n",
        "    SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=MISSING),\n",
        "    OrdinalEncoder(categories=categories),\n",
        ")\n",
        "\n",
        "preprocess_num_cols_2 = make_pipeline(\n",
        "    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n",
        "    StandardScaler(),\n",
        ")\n",
        "\n",
        "prepare_nonlinear = make_column_transformer(\n",
        "    (preprocess_num_cols_2, num_cols),\n",
        "    (preprocess_cat_cols_2, cat_cols),\n",
        "    remainder=\"drop\",\n",
        ")"
      ],
      "metadata": {
        "id": "yHoU0mDML4xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = make_pipeline(\n",
        "    prepare_nonlinear,\n",
        "    RandomForestRegressor(n_jobs=-1, random_state=42, min_weight_fraction_leaf=0.005),\n",
        ")\n",
        "rf.fit(X_full, y_full)\n",
        "\n",
        "# Get estimates\n",
        "y_hat = rf.predict(X_full)\n",
        "\n",
        "\n",
        "# Adjust object type\n",
        "\n",
        "y_full_col = pd.DataFrame({'SalePrice': np.exp(y_full)}, index = y_full.index)\n",
        "\n",
        "y_hat_col = pd.DataFrame({'SalePricePrediction': np.exp(y_hat)}, index = y_full.index)\n",
        "\n",
        "\n",
        "# Concatenate X_full, y_full_col, and y_hat_col\n",
        "df_combined = pd.concat([X_full, y_full_col, y_hat_col], axis=1)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "df_combined.to_csv('predictions_rf.csv', encoding = 'utf-8-sig')\n",
        "files.download('predictions_rf.csv')"
      ],
      "metadata": {
        "id": "dSzcdaZ8LMMW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}